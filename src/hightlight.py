from langchain.output_parsers import PydanticOutputParser
from langchain_core.exceptions import OutputParserException
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from langchain_core.prompts import PromptTemplate
from langchain.prompts import PromptTemplate
from typing import List, Optional, Dict
import time
import openai
import logging

class HighlightDocuments(BaseModel):
    """Return the specific part of a document used for answering the question."""
    keyword: List[str] = Field(
        ..., description="List of key phrases from answers and question"
    )


class GradeHallucinations(BaseModel):
    """Binary score for hallucination present in 'generation' answer."""
    Faithfulness_score: int = Field(..., description="Faithfulness_score like 1, 2, 3, 4, 5")

    
def hightLight_context(
    input_data,
    llm_model,
    sleep_time=5,
    ):
    system = """Extract key **terms (words or short phrases)** from the provided question and answer that are essential to understanding their meaning.
    Used documents: User question: <question>{question}</question> \n\n Generated answer: <answer>{generation}</answer>
    
    <format_instruction>
    {format_instructions}
    </format_instruction>
    """

    parser = PydanticOutputParser(pydantic_object=HighlightDocuments)

    prompt = PromptTemplate(
        template=system,
        input_variables=["documents", "question", "generation"],
        partial_variables={"format_instructions": parser.get_format_instructions()},
    )
    

    chain = prompt | llm_model | parser
    
    try:
        return chain.invoke(input_data)
    except openai.BadRequestError as e:
        logging.info(f"Too much requests, we are sleeping! \n the error is {e}")
        time.sleep(sleep_time)
        return hightLight_context(input_data=input_data)

    except openai.RateLimitError:
        logging.info("Too much requests exceeding rate limit, we are sleeping!")
        time.sleep(sleep_time)
        return hightLight_context(input_data=input_data)
        
    except OutputParserException as e:
        logging.error("Original OutputParserException: %s", e)
        pass
    
def format_docs(docs):
    """Formats a list of documents into a single string."""
    formatted_docs = ""
    for pmid, context in docs.items():
        formatted_docs += f"<doc>:\nsource: {pmid}\nContent: {context['abstract']}\n</doc>\n"
    return formatted_docs


def highlight_segments(output_abtract, lookup_response):
    keyword_list = lookup_response.keyword
    for keyword in keyword_list:
        if len(keyword) <=1:
            continue
        for source in output_abtract:
            highlighted_context = output_abtract[source]['abstract']
            if keyword in output_abtract[source]['abstract']:
                logging.info(f"找到 keyword: '{keyword}', 正在执行替换...")
                output_abtract[source]['abstract'] = highlighted_context.replace(keyword, f"<mark>{keyword}</mark>")
            
            elif keyword.lower() in output_abtract[source]['abstract']:
                output_abtract[source]['abstract'] = highlighted_context.replace(keyword.lower(), f"<mark>{keyword.lower()}</mark>")
            else:
                # logging.info(f"警告: 在 {source} context 中未找到 segment: '{keyword}'。未执行替换。")
                pass
    return output_abtract


def hallucination_test(llm_model, input_data):
    sleep_time = 3
    # LLM with function call
    # Prompt
    system = """# Role
    You are a meticulous evaluator assessing whether an answer generated by a Large Language Model (LLM) is factually grounded in the provided background information (context).

    # Task
    Your task is to determine if the information presented in the 'Generated Answer' can be sufficiently supported by the 'Retrieved Context'. You must also consider the relevance of the answer to the 'Original Query'. Provide a hallucination score, where a higher score indicates the answer is more likely fabricated or unsupported by the context (hallucinated).

    # Input Information
    Please evaluate the 'Generated Answer' strictly based **only** on the provided 'Retrieved Context'. Do not use any external knowledge.

    ## Retrieved Context: 
    <docs>{documents}</docs>
    
    ## Generated Answer:
    <generation>{generation}</generation>

    # Evaluation Criteria & Scoring Guide
    Evaluate based on the following criteria and provide an integer score from 1 to 5:

    *   **Score 1 (Fully Grounded & Relevant):** All key information in the answer is explicitly supported by the 'Retrieved Context' and is highly relevant to the 'Original Query' (assuming the context itself is relevant).
    *   **Score 2 (Mostly Grounded & Relevant):** The majority of key information is supported by the context. There might be minor, reasonable paraphrasing or synthesis, but no introduction of core facts not found in the context. Generally relevant to the query.
    *   **Score 3 (Partially Grounded / Mild Hallucination):** The answer contains a mix of information supported by the context and details or inferences not found there. Alternatively, the answer might be based on the context but has low relevance to the 'Original Query'.
    *   **Score 4 (Largely Hallucinated / Poorly Grounded):** Most of the key information in the answer cannot be verified from the 'Retrieved Context'. It appears largely fabricated by the model. This also applies if the context is irrelevant to the query, but the answer attempts to derive meaning from it anyway.
    *   **Score 5 (Completely Hallucinated / Fabricated / Irrelevant):** The information in the answer finds almost no support in the 'Retrieved Context'. This includes cases where the context is empty, but the answer provides specific information, or the answer is completely unrelated to both the context and the query.

    **Specific Handling Notes:**
    *   If the 'Retrieved Context' is **empty** or **completely irrelevant** to the 'Original Query', any 'Generated Answer' that provides specific, seemingly informative content should receive a **high score (4 or 5)**.
    *   If the 'Original Query' itself is **nonsensical** (e.g., random characters like "dfkj", "xxfckj"), and the 'Generated Answer' provides a seemingly coherent, specific response (i.e., not just stating inability to understand), it should also receive a **high score (4 or 5)**.
    *   **[NEW RULE]** If the 'Generated Answer' includes phrases explicitly stating that the required information is **missing** or **not mentioned** in the 'Retrieved Context' (e.g., "The context does not say...", "Based on the provided abstracts, there is no mention of the", "The provided documents do not contain details on..."), treat this as highly irrelevant or an inability to answer grounded in the context. Assign **Score 5**.

    Output format:
        <format_instruction>
        {format_instructions}
        <format_instruction>

    """
    parser = PydanticOutputParser(pydantic_object=GradeHallucinations)

    prompt = PromptTemplate(
        template=system,
        input_variables=["documents", "generation"],
        partial_variables={"format_instructions": parser.get_format_instructions()},
    )
    
    chain = prompt | llm_model | parser
    
    try:
        return chain.invoke(input_data)
    except openai.BadRequestError as e:
        logging.info(f"Too much requests, we are sleeping! \n the error is {e}")
        time.sleep(sleep_time)
        return hallucination_test(input_data=input_data)

    except openai.RateLimitError:
        logging.info("Too much requests exceeding rate limit, we are sleeping!")
        time.sleep(sleep_time)
        return hallucination_test(input_data=input_data)
        
    except OutputParserException as e:
        logging.error("Original OutputParserException: %s", e)
        pass
    
