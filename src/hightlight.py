from langchain.output_parsers import PydanticOutputParser
from langchain_core.exceptions import OutputParserException
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from langchain_core.prompts import PromptTemplate
from langchain.prompts import PromptTemplate
from typing import List, Optional, Dict
import time
import openai
import logging

class HighlightDocuments(BaseModel):
    """Return the specific part of a document used for answering the question."""
    keyword: List[str] = Field(
        ..., description="List of key phrases from answers and question"
    )


class GradeHallucinations(BaseModel):
    """Binary score for hallucination present in 'generation' answer."""
    Faithfulness_score: int = Field(..., description="Faithfulness_score like 1, 2, 3, 4, 5")

    
def hightLight_context(
    input_data,
    llm_model,
    sleep_time=5,
    ):
    system = """Extract key **terms (words or short phrases)** from the provided question and answer that are essential to understanding their meaning.
    Used documents: User question: <question>{question}</question> \n\n Generated answer: <answer>{generation}</answer>
    
    <format_instruction>
    {format_instructions}
    </format_instruction>
    """

    parser = PydanticOutputParser(pydantic_object=HighlightDocuments)

    prompt = PromptTemplate(
        template=system,
        input_variables=["documents", "question", "generation"],
        partial_variables={"format_instructions": parser.get_format_instructions()},
    )
    

    chain = prompt | llm_model | parser
    
    try:
        return chain.invoke(input_data)
    except openai.BadRequestError as e:
        logging.info(f"Too much requests, we are sleeping! \n the error is {e}")
        time.sleep(sleep_time)
        return hightLight_context(input_data=input_data)

    except openai.RateLimitError:
        logging.info("Too much requests exceeding rate limit, we are sleeping!")
        time.sleep(sleep_time)
        return hightLight_context(input_data=input_data)
        
    except OutputParserException as e:
        logging.error("Original OutputParserException: %s", e)
        pass
    
def format_docs(docs):
    """Formats a list of documents into a single string."""
    formatted_docs = ""
    for pmid, context in docs.items():
        formatted_docs += f"<doc>:\nsource: {pmid}\nContent: {context['abstract']}\n</doc>\n"
    return formatted_docs


def highlight_segments(output_abtract, lookup_response):
    keyword_list = lookup_response.keyword
    for keyword in keyword_list:
        if len(keyword) <=1:
            continue
        for source in output_abtract:
            highlighted_context = output_abtract[source]['abstract']
            if keyword in output_abtract[source]['abstract']:
                logging.info(f"找到 keyword: '{keyword}', 正在执行替换...")
                output_abtract[source]['abstract'] = highlighted_context.replace(keyword, f"<mark>{keyword}</mark>")
            
            elif keyword.lower() in output_abtract[source]['abstract']:
                output_abtract[source]['abstract'] = highlighted_context.replace(keyword.lower(), f"<mark>{keyword.lower()}</mark>")
            else:
                # logging.info(f"警告: 在 {source} context 中未找到 segment: '{keyword}'。未执行替换。")
                pass
    return output_abtract


def hallucination_test(llm_model, input_data):
    sleep_time = 3
    # LLM with function call
    # Prompt
    system = """You are a meticulous AI Content Evaluator.

    Task: Your primary task is to assess whether an answer generated by a Large Language Model (LLM) is strictly grounded in the provided 'Source Document'. You need to determine if the information presented in the 'Generated Answer' can be factually supported by the 'Source Document', assign a 'Faithfulness Score' based on this, and identify any 'hallucinated' content (information present in the answer but not supported by the document).
    
    Input:
    Documents: <docs>{documents}</docs>
    
    LLM Generation: <generation>{generation}</generation>
    
    Evaluation Criteria & Instructions:

    Strict Grounding: The core of the evaluation is to determine if every substantive claim in the 'Generated Answer' finds direct or clearly inferable support within the 'Source Document'. Do not use external knowledge or make assumptions beyond what is stated in the document.

    Faithfulness Score: Please provide a score from 1 to 5 based on the following criteria:

    5 (Fully Faithful): All information presented in the answer strictly derives from the source document. There are no additions, distortions, or omissions of critical information (relative to the scope the answer claims to cover).

    4 (Mostly Faithful): The vast majority of information comes from the source document. May contain very minor paraphrasing/inferences that don't alter core facts, or omit minimal non-critical details. No significant hallucinations.

    3 (Partially Faithful / Partially Hallucinated): Some information is grounded in the document, but the answer also contains significant unsupported information (hallucinations) or misinterprets the document's content noticeably.

    2 (Minimally Faithful / Mostly Hallucinated): Only a small portion of the answer can be traced back to the source document. Most content is unsupported (hallucinated) or clearly contradicts the document.

    1 (Not Faithful / Completely Hallucinated): The answer's information is almost entirely unsupported by the source document or completely misrepresents its content.

    Hallucination Identification: If the answer contains information not supported by the document ('hallucinations'), please explicitly list the specific phrases or sentences from the 'Generated Answer' that are unsupported.
    
    Output format:
    <format_instruction>
    {format_instructions}
    <format_instruction>
    """
    parser = PydanticOutputParser(pydantic_object=GradeHallucinations)

    prompt = PromptTemplate(
        template=system,
        input_variables=["documents", "generation"],
        partial_variables={"format_instructions": parser.get_format_instructions()},
    )
    
    chain = prompt | llm_model | parser
    
    try:
        return chain.invoke(input_data)
    except openai.BadRequestError as e:
        logging.info(f"Too much requests, we are sleeping! \n the error is {e}")
        time.sleep(sleep_time)
        return hallucination_test(input_data=input_data)

    except openai.RateLimitError:
        logging.info("Too much requests exceeding rate limit, we are sleeping!")
        time.sleep(sleep_time)
        return hallucination_test(input_data=input_data)
        
    except OutputParserException as e:
        logging.error("Original OutputParserException: %s", e)
        pass
    
